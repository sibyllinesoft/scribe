#!/usr/bin/env python3
"""
QA Runner - Production LLM-based Question-Answer Evaluation

Implements the core QA evaluation system according to TODO.md requirements:
- Real LLM integration with multiple providers
- Multi-seed evaluation with temperature control
- Prompt engineering with system/context/question structure
- Comprehensive logging and telemetry
- Budget enforcement and token tracking
- Statistical validation and reproducibility

Key Features:
- Pluggable LLM providers (OpenAI, Anthropic, local)
- Prompt immutability with SHA tracking
- Multi-seed runs for statistical validity
- Token budget enforcement
- Cost and latency monitoring
- Structured logging for analysis
"""

import asyncio
import json
import logging
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np

from .llm_client import LLMClient, LLMRequest, LLMResponse, create_llm_client
from ..prompts import get_prompt_registry, get_prompt, get_prompt_sha

logger = logging.getLogger(__name__)


@dataclass
class QATask:
    """A single QA task definition."""
    question_id: str
    question: str
    context_budget: int
    expected_answer: Optional[str] = None
    reference_patterns: Optional[List[str]] = None
    difficulty: str = "medium"
    category: str = "general"
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class QAAnswer:
    """Answer generated by LLM for a QA task."""
    question_id: str
    question: str
    answer: str
    model: str
    provider: str
    pack_variant: str
    context_tokens: int
    context_budget: int
    context_actual: str
    prompt_sha: str
    seed: int
    temperature: float
    response_metadata: Dict[str, Any]
    
    # Response metrics
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    cost_usd: float
    latency_ms: float
    timestamp: str
    request_id: str
    
    # Derived metrics
    context_efficiency: float = 0.0  # tokens used / budget
    
    def __post_init__(self):
        if self.context_budget > 0:
            self.context_efficiency = self.context_tokens / self.context_budget


@dataclass
class QARunConfig:
    """Configuration for QA evaluation run."""
    pack_paths: Dict[str, Path]  # variant_name -> pack_path
    tasks: List[QATask]
    llm_config: Dict[str, Any]
    seeds: List[int]
    temperature: float = 0.0
    max_tokens: Optional[int] = 2048
    output_dir: Path = Path("qa_outputs")
    log_level: str = "INFO"
    
    # Safety and validation
    enforce_budget: bool = True
    validate_prompts: bool = True
    max_concurrent: int = 3
    timeout_seconds: int = 300


class QARunner:
    """
    Production QA runner implementing TODO.md requirements.
    
    Manages LLM-based question-answering evaluation with:
    - Multi-provider LLM support
    - Prompt immutability enforcement  
    - Token budget management
    - Multi-seed reproducibility
    - Comprehensive telemetry
    """
    
    def __init__(self, config: QARunConfig):
        self.config = config
        self.output_dir = config.output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize LLM client
        self.llm_client = create_llm_client(
            config.llm_config,
            log_dir=self.output_dir / "llm_logs"
        )
        
        # Initialize prompt registry
        self.prompt_registry = get_prompt_registry()
        
        # Results storage
        self.results: List[QAAnswer] = []
        self.run_metadata = {
            "start_time": None,
            "end_time": None,
            "total_questions": len(config.tasks),
            "total_variants": len(config.pack_paths),
            "total_seeds": len(config.seeds),
            "prompt_manifest": {},
            "budget_violations": [],
            "errors": []
        }
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """Setup structured logging for the evaluation run."""
        log_file = self.output_dir / "qa_run.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config.log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    
    async def run_evaluation(self) -> Dict[str, Any]:
        """
        Run complete QA evaluation according to TODO.md specifications.
        
        Returns:
            Dictionary with evaluation results and statistics
        """
        logger.info("Starting QA evaluation run")
        self.run_metadata["start_time"] = datetime.utcnow().isoformat()
        
        # Validate setup
        await self._validate_setup()
        
        # Record prompt manifest for immutability
        self._record_prompt_manifest()
        
        try:
            # Run evaluation matrix
            await self._run_evaluation_matrix()
            
            # Generate summary statistics
            summary = self._generate_summary()
            
            # Save results
            await self._save_results()
            
            self.run_metadata["end_time"] = datetime.utcnow().isoformat()
            logger.info("QA evaluation completed successfully")
            
            return summary
            
        except Exception as e:
            logger.error(f"QA evaluation failed: {e}")
            self.run_metadata["errors"].append(str(e))
            raise
        finally:
            await self.llm_client.close()
    
    async def _validate_setup(self):
        """Validate that all required components are available."""
        # Check pack files exist
        for variant, pack_path in self.config.pack_paths.items():
            if not pack_path.exists():
                raise FileNotFoundError(f"Pack file not found for {variant}: {pack_path}")
        
        # Check prompt templates exist
        required_prompts = ["answerer_system", "answerer_user_template"]
        for prompt_name in required_prompts:
            if not self.prompt_registry.get_template(prompt_name):
                raise ValueError(f"Required prompt template missing: {prompt_name}")
        
        # Test LLM connectivity
        test_response = await self.llm_client.generate(
            prompt="Test connectivity",
            temperature=0.0,
            max_tokens=10
        )
        logger.info(f"LLM connectivity verified: {test_response.model}")
    
    def _record_prompt_manifest(self):
        """Record prompt manifest for immutability validation."""
        manifest = self.prompt_registry.get_manifest()
        self.run_metadata["prompt_manifest"] = manifest
        
        # Save manifest to file
        manifest_file = self.output_dir / "prompt_manifest.json"
        with open(manifest_file, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        logger.info(f"Recorded prompt manifest with {len(manifest)} templates")
    
    async def _run_evaluation_matrix(self):
        """Run the full evaluation matrix: variants x tasks x seeds."""
        total_runs = len(self.config.pack_paths) * len(self.config.tasks) * len(self.config.seeds)
        logger.info(f"Running {total_runs} QA evaluations")
        
        # Create semaphore for concurrent control
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        # Generate all evaluation tasks
        evaluation_tasks = []
        for variant_name, pack_path in self.config.pack_paths.items():
            for task in self.config.tasks:
                for seed in self.config.seeds:
                    evaluation_tasks.append(
                        self._run_single_qa(semaphore, variant_name, pack_path, task, seed)
                    )
        
        # Execute all tasks concurrently
        results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)
        
        # Process results and collect errors
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Task {i} failed: {result}")
                self.run_metadata["errors"].append(str(result))
            else:
                self.results.append(result)
        
        logger.info(f"Completed {len(self.results)} successful QA evaluations")
    
    async def _run_single_qa(
        self, 
        semaphore: asyncio.Semaphore,
        variant_name: str,
        pack_path: Path, 
        task: QATask,
        seed: int
    ) -> QAAnswer:
        """Run a single QA evaluation."""
        async with semaphore:
            start_time = time.time()
            
            # Load pack content
            pack_content = self._load_pack_content(pack_path)
            
            # Apply context budget
            trimmed_context, actual_tokens = self._apply_context_budget(
                pack_content, task.context_budget
            )
            
            # Check budget enforcement
            if self.config.enforce_budget and actual_tokens > task.context_budget:
                violation = {
                    "variant": variant_name,
                    "question_id": task.question_id,
                    "budget": task.context_budget,
                    "actual": actual_tokens,
                    "overage": actual_tokens - task.context_budget
                }
                self.run_metadata["budget_violations"].append(violation)
                logger.warning(f"Budget violation: {task.question_id} used {actual_tokens}/{task.context_budget} tokens")
            
            # Build prompt
            prompt = self._build_qa_prompt(task.question, trimmed_context)
            prompt_sha = hashlib.sha256(prompt.encode()).hexdigest()
            
            # Generate answer
            response = await self.llm_client.generate(
                prompt=prompt,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                seed=seed
            )
            
            # Create answer record
            answer = QAAnswer(
                question_id=task.question_id,
                question=task.question,
                answer=response.text,
                model=response.model,
                provider=response.metadata.get("provider", "unknown"),
                pack_variant=variant_name,
                context_tokens=actual_tokens,
                context_budget=task.context_budget,
                context_actual=trimmed_context[:500] + "..." if len(trimmed_context) > 500 else trimmed_context,
                prompt_sha=prompt_sha,
                seed=seed,
                temperature=self.config.temperature,
                response_metadata=response.metadata or {},
                prompt_tokens=response.prompt_tokens,
                completion_tokens=response.completion_tokens,
                total_tokens=response.total_tokens,
                cost_usd=response.cost_usd,
                latency_ms=response.latency_ms,
                timestamp=response.timestamp,
                request_id=response.request_id
            )
            
            logger.debug(f"Completed QA: {variant_name}/{task.question_id}/seed-{seed} in {response.latency_ms:.1f}ms")
            return answer
    
    def _load_pack_content(self, pack_path: Path) -> str:
        """Load content from pack file."""
        try:
            with open(pack_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # If it's JSON format, extract the body
            try:
                pack_data = json.loads(content)
                if isinstance(pack_data, dict) and "body" in pack_data:
                    return pack_data["body"]
                elif isinstance(pack_data, dict) and "content" in pack_data:
                    return pack_data["content"]
                else:
                    return content
            except json.JSONDecodeError:
                # Not JSON, use as plain text
                return content
                
        except Exception as e:
            logger.error(f"Failed to load pack {pack_path}: {e}")
            return ""
    
    def _apply_context_budget(self, content: str, budget: int) -> Tuple[str, int]:
        """
        Apply context budget by trimming content to fit within token limit.
        
        Args:
            content: Full pack content
            budget: Maximum tokens allowed
            
        Returns:
            Tuple of (trimmed_content, actual_tokens)
        """
        # Estimate tokens (rough approximation)
        estimated_tokens = len(content) // 4  # ~4 chars per token
        
        if estimated_tokens <= budget:
            return content, estimated_tokens
        
        # Trim content to fit budget
        # Use character-based trimming as approximation
        target_chars = budget * 4
        
        if len(content) <= target_chars:
            return content, len(content) // 4
        
        # Try to trim at natural boundaries (paragraphs, sections)
        trimmed = content[:target_chars]
        
        # Find last paragraph or section break
        last_break = max(
            trimmed.rfind('\n\n'),
            trimmed.rfind('\n### '),
            trimmed.rfind('\n## ')
        )
        
        if last_break > target_chars * 0.8:  # Only if we don't lose too much content
            trimmed = trimmed[:last_break]
        
        actual_tokens = len(trimmed) // 4
        return trimmed, actual_tokens
    
    def _build_qa_prompt(self, question: str, context: str) -> str:
        """Build QA prompt using system and user templates."""
        # Get system prompt
        system_prompt = get_prompt("answerer_system")
        
        # Get user prompt with context and question
        user_prompt = get_prompt(
            "answerer_user_template", 
            context=context,
            question=question
        )
        
        # Combine system and user prompts
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return full_prompt
    
    def _generate_summary(self) -> Dict[str, Any]:
        """Generate summary statistics for the evaluation run."""
        if not self.results:
            return {"error": "No successful evaluations completed"}
        
        # Basic statistics
        total_cost = sum(r.cost_usd for r in self.results)
        total_tokens = sum(r.total_tokens for r in self.results)
        avg_latency = np.mean([r.latency_ms for r in self.results])
        
        # Variant statistics
        variant_stats = {}
        for variant in self.config.pack_paths.keys():
            variant_results = [r for r in self.results if r.pack_variant == variant]
            if variant_results:
                variant_stats[variant] = {
                    "evaluations": len(variant_results),
                    "avg_latency_ms": np.mean([r.latency_ms for r in variant_results]),
                    "total_cost_usd": sum(r.cost_usd for r in variant_results),
                    "total_tokens": sum(r.total_tokens for r in variant_results),
                    "avg_context_efficiency": np.mean([r.context_efficiency for r in variant_results])
                }
        
        # Provider statistics
        provider_stats = {}
        for provider in set(r.provider for r in self.results):
            provider_results = [r for r in self.results if r.provider == provider]
            provider_stats[provider] = {
                "evaluations": len(provider_results),
                "total_cost_usd": sum(r.cost_usd for r in provider_results),
                "avg_latency_ms": np.mean([r.latency_ms for r in provider_results])
            }
        
        summary = {
            "run_metadata": self.run_metadata,
            "overall_stats": {
                "total_evaluations": len(self.results),
                "total_cost_usd": total_cost,
                "total_tokens": total_tokens,
                "avg_latency_ms": avg_latency,
                "success_rate": len(self.results) / (len(self.config.pack_paths) * len(self.config.tasks) * len(self.config.seeds)),
                "budget_violations": len(self.run_metadata["budget_violations"]),
                "errors": len(self.run_metadata["errors"])
            },
            "variant_stats": variant_stats,
            "provider_stats": provider_stats,
            "config": {
                "variants": list(self.config.pack_paths.keys()),
                "tasks": len(self.config.tasks),
                "seeds": self.config.seeds,
                "temperature": self.config.temperature,
                "enforce_budget": self.config.enforce_budget
            }
        }
        
        return summary
    
    async def _save_results(self):
        """Save evaluation results to files."""
        # Save individual answers
        answers_file = self.output_dir / "qa_answers.jsonl"
        with open(answers_file, 'w') as f:
            for answer in self.results:
                f.write(json.dumps(asdict(answer)) + '\n')
        
        # Save summary
        summary = self._generate_summary()
        summary_file = self.output_dir / "qa_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Save raw results for reproducibility
        raw_file = self.output_dir / "qa_raw_results.json"
        raw_data = {
            "config": {
                "pack_paths": {k: str(v) for k, v in self.config.pack_paths.items()},
                "seeds": self.config.seeds,
                "temperature": self.config.temperature,
                "llm_config": self.config.llm_config
            },
            "tasks": [asdict(task) for task in self.config.tasks],
            "results": [asdict(answer) for answer in self.results],
            "metadata": self.run_metadata
        }
        
        with open(raw_file, 'w') as f:
            json.dump(raw_data, f, indent=2)
        
        logger.info(f"Results saved to {self.output_dir}")


async def run_qa_evaluation(config: QARunConfig) -> Dict[str, Any]:
    """
    Main entry point for running QA evaluation.
    
    Args:
        config: QA run configuration
        
    Returns:
        Evaluation results dictionary
    """
    runner = QARunner(config)
    return await runner.run_evaluation()


# CLI for running evaluations
async def main():
    """CLI entry point for QA runner."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Run PackRepo QA Evaluation")
    parser.add_argument("--config", required=True, help="Configuration JSON file")
    parser.add_argument("--output-dir", default="qa_outputs", help="Output directory")
    parser.add_argument("--log-level", default="INFO", help="Log level")
    args = parser.parse_args()
    
    # Load configuration
    with open(args.config) as f:
        config_data = json.load(f)
    
    # Create config object
    config = QARunConfig(
        pack_paths={k: Path(v) for k, v in config_data["pack_paths"].items()},
        tasks=[QATask(**task) for task in config_data["tasks"]],
        llm_config=config_data["llm_config"],
        seeds=config_data.get("seeds", [42]),
        temperature=config_data.get("temperature", 0.0),
        max_tokens=config_data.get("max_tokens", 2048),
        output_dir=Path(args.output_dir),
        log_level=args.log_level
    )
    
    # Run evaluation
    try:
        results = await run_qa_evaluation(config)
        print(f"\n{'='*60}")
        print("QA EVALUATION COMPLETED")
        print(f"{'='*60}")
        print(f"Total Evaluations: {results['overall_stats']['total_evaluations']}")
        print(f"Success Rate: {results['overall_stats']['success_rate']:.2%}")
        print(f"Total Cost: ${results['overall_stats']['total_cost_usd']:.4f}")
        print(f"Average Latency: {results['overall_stats']['avg_latency_ms']:.1f}ms")
        print(f"Budget Violations: {results['overall_stats']['budget_violations']}")
        print(f"Results saved to: {args.output_dir}")
        print(f"{'='*60}")
        
    except Exception as e:
        print(f"Evaluation failed: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))